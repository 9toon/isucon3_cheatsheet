Please see [09.unicorn.md](09.unicorn.md) for unicorn's detail. 

# nginx + unicorn (sinatra)

cf. http://recipes.sinatrarb.com/p/deployment/nginx_proxied_to_unicorn

$ sudo yum install nginx # これだと最新版じゃないんだけど ISUCON の最中にビルド時間とかもったいないしなぁ...

### unicorn.conf (GC無効)

listen のオプション. cf. http://unicorn.bogomips.org/Unicorn/Configurator.html#method-i-listen

* :backlog => number of clients

    * default 1024. sockfd についての保留中の接続のキューの最大長を指定する。 キューがいっぱいの状態で接続要求が到着すると、クライアントは ECONNREFUSED というエラーを受け取る。cf. http://linuxjm.sourceforge.jp/html/LDP_man-pages/man2/listen.2.html

* :rcvbuf => bytes, :sndbuf => bytes

    * Linux 2.4+ have intelligent auto-tuning mechanisms and there is no need to specify them

* :tcp_no_delay => true or false

    * write時にos/kernelレベルでbufferingしてたまってからsendするのをやめて即座にsendします.
    * 小さいパケットはまとめて、それからwriteするようにしましょう. 
    * cf. kazeburo yapc 2013 http://www.slideshare.net/kazeburo/yapc2013psgi-plack

* :tcp_nopush => true or false

    * default false. true にすべき(あれ、今はデフォルトtrueじゃなかったけ？)
    * cf. kazeburo G-WAN はなぜ速いのか http://blog.nomadscafe.jp/2013/09/benchmark-g-wan-and-nginx.html

* :tries => seconds

    * Times to retry binding a socket if it is already in use. Default 5 seconds.

* :delays => seconds

    * Seconds to wait between successive tries. Default: 0.5 seconds

* :tcp_defer_accept => integer

    * Default 1. コネクションが完了したタイミングではなくデータが到着した段階でプロセスを起こします.
    * プリフォーク型のhttpdにおいて処理中となるプロセス数を減らすテクニック
    * cf.kazeburo yapc 2013http://www.slideshare.net/kazeburo/yapc2013psgi-plack

unicorn.conf

```ruby
@dir = "/home/game/gitrepos/isucon2/webapp/ruby/"
working_directory @dir

listen "/tmp/app.sock", backlog: 1024
pid "#{@dir}tmp/pids/unicorn.pid"
stderr_path "#{@dir}log/unicorn.stderr.log"
stdout_path "#{@dir}log/unicorn.stdout.log"

worker_processes 50 # CPUの数 * 2 ぐらい？
timeout 60 # default: 60

preload_app true # allow copy-on-write-friendly GC to save memory
GC.respond_to?(:copy_on_write_friendly=) and GC.copy_on_write_friendly = true

after_fork do |server, worker|
  GC.disable
end

# Below is for ActiveRecord
=begin
before_fork do |server, worker|
  defined?(ActiveRecord::Base) and ActiveRecord::Base.connection.disconnect!
  old_pid = "#{server.config[:pid]}.oldbin"
  if old_pid != server.pid
    begin
      sig = (worker.nr + 1) >= server.worker_processes ? :QUIT : :TTOU
      Process.kill(sig, File.read(old_pid).to_i)
    rescue Errno::ENOENT, Errno::ESRCH
    end
  end
  sleep 1
end

after_fork do |server, worker|
  defined?(ActiveRecord::Base) and ActiveRecord::Base.establish_connection
end
=end
```

### Gemfile

```
gem 'unicorn-worker-killer' # https://github.com/kzk/unicorn-worker-killer
```

### configu.ru

```ruby
require 'unicorn/oob_gc'
use Unicorn::OobGC, 10 # run GC each 10 requests

require 'unicorn/worker_killer'
# Max Requests per Worker
use Unicorn::WorkerKiller::MaxRequests, 3072, 4096
# Max Memory size (RSS) per worker
use Unicorn::WorkerKiller::Oom, (192*(1024**2)), (256*(1024**2))
```

### /etc/nginx/nginx.conf

* http://recipes.sinatrarb.com/p/deployment/nginx_proxied_to_unicorn
* http://nginx.org/en/docs/http/ngx_http_core_module.html
* 追記: kazeburo.conf  https://github.com/kazeburo/isucon3qualifier-myhack/blob/master/conf/nginx.conf
* 追記：こちらのサイトも参考になる！ http://nodejs.osser.jp/thread/53440873b986bc810f3930e1

```
user nginx; # this sets the user nginx will run as,
worker_rlimit_nofile 65535; # limit of number of open file descriptoers, should be >= 4 * worker_connections
worker_processes  4; # no of cpu cores
#worker_cpu_affinity 0010 1000;
pcre_jit on; # Just In Compile regular expression on conf

error_log  /var/log/nginx/error.log; # setup where nginx will log errors to
pid        /var/run/nginx.pid; # and where the nginx process id resides

events {
  use epoll;
  worker_connections 10000;
  accept_mutex on; # set to on if you have more than 1 worker_processes
  #accept_mutex_delay 500ms;
}

http {
  include /etc/nginx/mime.types; # text/css, text/javascript
  default_type           text/html;
  sendfile               on; # use the kernel sendfile
  tcp_nopush             on; # prepend http headers before sendfile(), should be on
  tcp_nodelay            on; # on for keepalive?
  send_timeout           60; # クライアントへの応答の送信タイムアウト
  keepalive_timeout       0; # 0 to disable keepalive. ケースバイケースなので考えて切り替える
  # keepalive_timeout      60;
  # keepalive_requests   3000;

  client_header_timeout        5;
  client_body_timeout          30;
  client_body_temp_path        /dev/shm/client_temp 1 1;
  client_max_body_size         10m;
  client_body_buffer_size      32k;
  client_header_buffer_size    2k;
  # reset_timedout_connection on; # 非アクティブクライアントの connection をクローズする
  large_client_header_buffers  4 8k;
  proxy_connect_timeout 5;
  proxy_send_timeout    5;
  proxy_read_timeout    60;
  proxy_buffering off;
  proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;
  proxy_temp_path /dev/shm/proxy_temp 1 1;
  proxy_cache_path /dev/shm/cache levels=1:2 keys_zone=cache-space:20m max_size=300m inactive=10m;

  gzip  on; # gzip 圧縮. cpu を食うトレードオフ. クライアントによってはおかしくなることがあったと思う
  gzip_vary on;
  gzip_min_length 500;
  gzip_disable "MSIE [1-6]\.(?!.*SV1)";
  gzip_types text/plain text/xml text/css
     text/comma-separated-values
     text/javascript application/x-javascript
     application/atom+xml image/x-icon;

  log_format  ltsv  'host:$remote_addr\t'
                    'vhost:$http_host\t'
                    'port:$server_port\t'
                    'time:$time_iso8601\t'
                    'method:$request_method\t'
                    'uri:$request_uri\t'
                    'protocol:$server_protocol\t'
                    'status:$status\t'
                    'size:$body_bytes_sent\t'
                    'ua:$http_user_agent\t'
                    'forwardedfor:$http_x_forwarded_for\t'
                    'forwardedproto:$http_x_forwarded_proto\t'
                    'apptime:$upstream_response_time\t'
                    'reqtime:$request_time';

  # access_log /var/log/nginx/access.log ltsv;
  access_log off;

  # use the socket we configured in our unicorn.rb
  # http://wiki.nginx.org/HttpUpstreamModule
  upstream apps {
    server unix:/tmp/app.sock
        fail_timeout=0; # 休み時間なし
        # max_fails=2 fail_timeout=10; # ２回失敗したら10分休み. For load balancing
  }

  # configure the virtual host
  server {
    # replace this with your static Sinatra app files, root + public
    root /home/isucon/webapp/public;
    # port to listen for requests on
    listen 5000;
    # maximum accepted body size of client request
    client_max_body_size 4G;

    location / {
      ## if static file exists, return it. Otherwrise, fallback to @webapp
      try_files $uri @webapp; # 一度探しにいくからちょっと遅くなるか？
    }
    
    location @webapp {
      #proxy_http_version 1.1;
      #proxy_set_header Connection "";
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header Host $http_host;
      proxy_redirect off;
      proxy_pass http://apps; # pass to the upstream apps server
    }

    location = /favicon.ico {
      open_file_cache max=100;
      root /home/isucon/webapp/public;
    }

    location ~ ^/(img|css|js)/ {
      #gzip_static  on; 
      #gzip_types text/css application/x-javascript;
      open_file_cache max=100;
      # open_file_cache max=100000 inactive=20s # キャッシュファイル最大数とキャッシュ時間
      # open_file_cache_valid 30s; # チェック間隔
      # open_file_cache_min_uses 2; # 非アクティブファイルの最小ファイル数
      # open_file_cache_errors on; # ファイルのエラーもキャッシュする
      root /home/isucon/webapp/public;
    }
  }
}
```

うーん、796 tikets から 432 tickets に落ちた ... => アプリに処理を回しすぎて 499 になってしまうらしい。アプリを改善しないと始まらない。

# nginx + unicorn (rails)

cf. http://qiita.com/y_uuki/items/118bce7b3a4acb545d79
